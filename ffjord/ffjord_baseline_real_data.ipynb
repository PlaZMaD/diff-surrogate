{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: Failing to collect the installed os packages\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/schattengenie/diff-sim-ffjord/b2852c1881f24e158b30cb86dea8b948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import comet_ml\n",
    "from comet_ml import API\n",
    "from comet_ml import Experiment\n",
    "\n",
    "experiment = Experiment('HU8yNOX96Ang8huavKsvrTbiK', project_name=\"diff_sim_ffjord\", workspace=\"schattengenie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from model import YModel\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "%pylab inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovering conditional density with FFJORD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All needed imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/rtqichen/torchdiffeq.git\n",
      "  Cloning https://github.com/rtqichen/torchdiffeq.git to /tmp/pip-req-build-5jredbvr\n",
      "  Running command git clone -q https://github.com/rtqichen/torchdiffeq.git /tmp/pip-req-build-5jredbvr\n",
      "Requirement already satisfied (use --upgrade to upgrade): torchdiffeq==0.0.1 from git+https://github.com/rtqichen/torchdiffeq.git in /root/miniconda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: torch>=0.4.1 in /root/miniconda/lib/python3.6/site-packages (from torchdiffeq==0.0.1) (1.1.0)\n",
      "Requirement already satisfied: numpy in /root/miniconda/lib/python3.6/site-packages (from torch>=0.4.1->torchdiffeq==0.0.1) (1.16.3)\n",
      "Building wheels for collected packages: torchdiffeq\n",
      "  Building wheel for torchdiffeq (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-fu7w4n5u/wheels/f1/89/ce/78b4c1aabbb8dad56a2dbd776f9ffcbeca103b2ddae40d094b\n",
      "Successfully built torchdiffeq\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/rtqichen/torchdiffeq.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n",
    "\n",
    "sys.path.append('./ffjord/')\n",
    "import ffjord.lib.utils as utils\n",
    "from ffjord.lib.visualize_flow import visualize_transform\n",
    "import ffjord.lib.layers.odefunc as odefunc\n",
    "from ffjord.train_misc import standard_normal_logprob\n",
    "from ffjord.train_misc import count_nfe, count_parameters, count_total_time\n",
    "from ffjord.train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
    "from ffjord.train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
    "from ffjord.train_misc import build_model_tabular\n",
    "import lib.layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tanh': Tanh(), 'relu': ReLU(), 'softplus': Softplus(beta=1, threshold=20), 'elu': ELU(alpha=1.0), 'swish': Swish(), 'square': Lambda(), 'identity': Lambda()}\n"
     ]
    }
   ],
   "source": [
    "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams', 'fixed_adams']\n",
    "\n",
    "print(odefunc.NONLINEARITIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cnf_options(model, solver, rademacher, residual, atol=1e-4, rtol=1e-4):\n",
    "\n",
    "    def _set(module):\n",
    "        if isinstance(module, layers.CNF):\n",
    "            # Set training settings\n",
    "            module.solver = solver\n",
    "            module.atol = atol\n",
    "            module.rtol = rtol\n",
    "\n",
    "            # If using fixed-grid adams, restrict order to not be too high.\n",
    "            if solver in ['fixed_adams', 'explicit_adams']:\n",
    "                module.solver_options['max_order'] = 4\n",
    "\n",
    "        if isinstance(module, layers.ODEfunc):\n",
    "            module.rademacher = rademacher\n",
    "            module.residual = residual\n",
    "\n",
    "    model.apply(_set)\n",
    "    \n",
    "# layer_type - [\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
    "def build_model_tabular(dims=2,\n",
    "                        layer_type='concatsquash', \n",
    "                        nonlinearity='relu', \n",
    "                        residual=False, \n",
    "                        rademacher=False,\n",
    "                        train_T=True,\n",
    "                        solver='dopri5',\n",
    "                        time_length=0.1,\n",
    "                        divergence_fn='approximate', # [\"brute_force\", \"approximate\"]\n",
    "                        hidden_dims=(32, 32), \n",
    "                        num_blocks=1, batch_norm=False, \n",
    "                        bn_lag=0, regularization_fns=None):\n",
    "\n",
    "\n",
    "    def build_cnf():\n",
    "        diffeq = layers.ODEnet(\n",
    "            hidden_dims=hidden_dims,\n",
    "            input_shape=(dims,),\n",
    "            strides=None,\n",
    "            conv=False,\n",
    "            layer_type=layer_type,\n",
    "            nonlinearity=nonlinearity,\n",
    "        )\n",
    "        odefunc = layers.ODEfunc(\n",
    "            diffeq=diffeq,\n",
    "            divergence_fn=divergence_fn,\n",
    "            residual=residual,\n",
    "            rademacher=rademacher,\n",
    "        )\n",
    "        cnf = layers.CNF(\n",
    "            odefunc=odefunc,\n",
    "            T=time_length,\n",
    "            train_T=train_T,\n",
    "            regularization_fns=regularization_fns,\n",
    "            solver=solver,\n",
    "        )\n",
    "        return cnf\n",
    "\n",
    "    chain = [build_cnf() for _ in range(num_blocks)]\n",
    "    if batch_norm:\n",
    "        bn_layers = [layers.MovingBatchNorm1d(dims, bn_lag=bn_lag) for _ in range(num_blocks)]\n",
    "        bn_chain = [layers.MovingBatchNorm1d(dims, bn_lag=bn_lag)]\n",
    "        for a, b in zip(chain, bn_layers):\n",
    "            bn_chain.append(a)\n",
    "            bn_chain.append(b)\n",
    "        chain = bn_chain\n",
    "    model = layers.SequentialFlow(chain)\n",
    "\n",
    "    set_cnf_options(model, solver, rademacher, residual)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffjord.lib.layers.wrappers.cnf_regularization as reg_lib\n",
    "import six\n",
    "\n",
    "REGULARIZATION_FNS = {\n",
    "    \"l1int\": reg_lib.l1_regularzation_fn,\n",
    "    \"l2int\": reg_lib.l2_regularzation_fn,\n",
    "    \"dl2int\": reg_lib.directional_l2_regularization_fn,\n",
    "    \"JFrobint\": reg_lib.jacobian_frobenius_regularization_fn,\n",
    "    \"JdiagFrobint\": reg_lib.jacobian_diag_frobenius_regularization_fn,\n",
    "    \"JoffdiagFrobint\": reg_lib.jacobian_offdiag_frobenius_regularization_fn,\n",
    "}\n",
    "\n",
    "def create_regularization_fns(regs={'l1int': 1., 'JFrobint': 1.}):\n",
    "    regularization_fns = []\n",
    "    regularization_coeffs = []\n",
    "\n",
    "    for arg_key, reg_fn in six.iteritems(REGULARIZATION_FNS):\n",
    "        if arg_key in regs:\n",
    "            regularization_fns.append(reg_fn)\n",
    "            regularization_coeffs.append(regs[arg_key])\n",
    "\n",
    "    regularization_fns = tuple(regularization_fns)\n",
    "    regularization_coeffs = tuple(regularization_coeffs)\n",
    "    return regularization_fns, regularization_coeffs\n",
    "\n",
    "\n",
    "def get_regularization(model, regularization_coeffs):\n",
    "    if len(regularization_coeffs) == 0:\n",
    "        return None\n",
    "\n",
    "    acc_reg_states = tuple([0.] * len(regularization_coeffs))\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, layers.CNF):\n",
    "            acc_reg_states = tuple(acc + reg for acc, reg in zip(acc_reg_states, module.get_regularization_states()))\n",
    "    return acc_reg_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(model):\n",
    "\n",
    "    def sample_fn(z, logpz=None):\n",
    "        if logpz is not None:\n",
    "            return model(z, logpz, reverse=True)\n",
    "        else:\n",
    "            return model(z, reverse=True)\n",
    "\n",
    "    def density_fn(x, logpx=None):\n",
    "        if logpx is not None:\n",
    "            return model(x, logpx, reverse=False)\n",
    "        else:\n",
    "            return model(x, reverse=False)\n",
    "\n",
    "    return sample_fn, density_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./simple_surr.csv')\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df = df.loc[df.magn_len == 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cart2sph(x, y, z):\n",
    "    hxy = np.hypot(x, y)\n",
    "    r = np.hypot(hxy, z)\n",
    "    el = np.arctan2(z, hxy)\n",
    "    az = np.arctan2(y, x)\n",
    "    return az, np.pi / 2 - el, r\n",
    "start_theta = df.start_theta.values\n",
    "start_phi = df.start_phi.values\n",
    "az, el, r = cart2sph(df.start_px.values, df.start_py.values, df.start_pz.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sqrt(df[['start_px', 'start_py', 'start_pz']].pow(2).sum(axis=1).values) 'magn_len'\n",
    "# init_cond = np.c_[, df[['start_theta', 'start_phi']].values]\n",
    "result = df[['hit_x', 'hit_y']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(result[:, 0], bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'init_cond' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-acf52ac4709a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minit_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'init_cond' is not defined"
     ]
    }
   ],
   "source": [
    "init_cond.shape, result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = result / 500 # np.concatenate([init_cond, result], axis=1)\n",
    "data = torch.tensor(data).to(device).float()\n",
    "# data = data / torch.tensor([1., 0.03, 2., 500., 100.]).to(device)\n",
    "# data = data / torch.tensor([100., 100.]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.std(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining FFJORD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams', 'fixed_adams']\n",
    "# layer_type - [\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization_fns = None\n",
    "#regularization_fns, regularization_coeffs = create_regularization_fns()\n",
    "model = build_model_tabular(dims=data.size(1),       \n",
    "                            layer_type='concatsquash',\n",
    "                            num_blocks=2,\n",
    "                            time_length=.5,\n",
    "                            rademacher=False, # descrete distr?\n",
    "                            nonlinearity='tanh',\n",
    "                            solver='rk4',\n",
    "                            hidden_dims=(32, 32, 32), \n",
    "                            batch_norm=False,\n",
    "                            regularization_fns=regularization_fns).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def standard_normal_logprob(z):\n",
    "    logZ = -0.5 * math.log(2 * math.pi)\n",
    "    return logZ - z.pow(2) / 2\n",
    "\n",
    "def compute_loss(model, data, batch_size=None):\n",
    "    zero = torch.zeros(data.shape[0], 1).to(data.device)\n",
    "    z, delta_logp = model(data, zero)\n",
    "\n",
    "    # compute log q(z)\n",
    "    logpz = standard_normal_logprob(z)\n",
    "\n",
    "    logpx = logpz.sum(1, keepdim=True) - delta_logp\n",
    "    loss = -torch.mean(logpx)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm\n",
    "from tqdm import tqdm\n",
    "from torchcontrib.optim import SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_base = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "optimizer = SWA(optimizer_base, swa_start=200, swa_freq=30, swa_lr=1e-5)\n",
    "\n",
    "loss_meter = utils.RunningAverageMeter(0.5)\n",
    "nfef_meter = utils.RunningAverageMeter(0.5)\n",
    "nfeb_meter = utils.RunningAverageMeter(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "model.train();\n",
    "losses = []\n",
    "for i in tqdm(range(10000)):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # loss, loss_xy = compute_loss(model, data) # [torch.randperm(len(data))[:B]])\n",
    "    loss = compute_loss(model, data[torch.randperm(len(data))[:B]])\n",
    "\n",
    "    loss_meter.update(loss.item())\n",
    "    # nfe_forward = count_nfe(model)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    nfe_total = count_nfe(model)\n",
    "    # nfe_backward = nfe_total - nfe_forward\n",
    "    # nfef_meter.update(nfe_forward)\n",
    "    # nfeb_meter.update(nfe_backward)\n",
    "    clip_grad_norm(model.parameters(), 5)\n",
    "    losses.append(loss_meter.avg)\n",
    "    if i % 50 == 0:\n",
    "        clear_output()\n",
    "        plt.figure()\n",
    "        plt.plot(losses)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "sample_fn, density_fn = get_transforms(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled = sample_fn(torch.randn(1000, 2).float().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.hist(data[:, i].detach().cpu().numpy(), bins=100, label='true', alpha=0.5)\n",
    "    plt.hist(sampled[:, i].detach().cpu().numpy(), bins=100, label='sampled', alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from pyro import distributions as dist\n",
    "my_cmap = plt.cm.jet\n",
    "my_cmap.set_under('white')\n",
    "mu_range = (1, 14)\n",
    "mu = torch.linspace(*mu_range, 20).view(-1, 1).to(device)\n",
    "N = 5000\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in tqdm(range(len(mu))):\n",
    "    mu_r = mu[i, :].reshape(1, -1).repeat(N, 1).to(device)\n",
    "    init_ = dist.Uniform(low=torch.tensor([5., 0., -np.pi]), high=torch.tensor([10., 0.09, np.pi])).sample((N,)).to(device)\n",
    "    inputs_test = torch.cat([\n",
    "        torch.randn(len(mu_r), 2).float().to(device)\n",
    "    ], dim=1)\n",
    "    sampled_data = sample_fn(inputs_test)\n",
    "    results.append(sampled_data)\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# energy, theta, phi, mu, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    plt.scatter(*result[:, [4, 5]].t().cpu().detach().numpy())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*df.loc[df.magn_len == 8][['hit_y', 'hit_x']].values.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*data[:, [4, 5]].detach().cpu().numpy().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_cond.min(axis=0), init_cond.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
